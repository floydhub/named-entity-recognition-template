{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "\n",
    "Hi üôÇ, if you are seeing this notebook, you have succesfully started your first project on FloydHub üöÄ, hooray!!\n",
    "\n",
    "[Named Enity Recognition](https://en.wikipedia.org/wiki/Named-entity_recognition) is one of the most common [NLP](https://en.wikipedia.org/wiki/Natural-language_processing) problems. The goal is classify named entities in text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.\n",
    "*What can you use it for?* Here are a few ideas - social media, chatbot, customer support tickets, survey responses, and data mining! \n",
    "\n",
    "### Predicting named entities of GMB(Groningen Meaning Bank) corpus\n",
    "\n",
    "In this notebook we will perform a [Sequence Tagging with a LSTM-CRF model](https://www.depends-on-the-definition.com/sequence-tagging-lstm-crf/) to extract the named entities from the annotated corpus.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/floydhub/named-entity-recognition-template/master/images/ner-image.png\" width=\"800\" height=\"800\" align=\"center\"/>\n",
    "\n",
    "Entity tags are encoded using a BIO annotation scheme, where each entity label is prefixed with either B or I letter. B- denotes the beginning and I- inside of an entity. The prefixes are used to detect multiword entities, e.g. sentence:\"World War II\", tags:(B-eve, I-eve, I-eve). All other words, which don‚Äôt refer to entities of interest, are labeled with the O tag.\n",
    "\n",
    "Tag | Label meaning | Example Given\n",
    "------------ | ------------- | \n",
    "geo | Geographical Entity | London\n",
    "org | Organization | ONU\n",
    "per | Person | Bush\n",
    "gpe | Geopolitical Entity | British\n",
    "tim | Time indicator | Wednesday\n",
    "art | Artifact | Chrysler\n",
    "eve | Event | Christmas\n",
    "nat | Natural Phenomenon | Hurricane\n",
    "O | No-Label | the\n",
    "\n",
    "We will:\n",
    "- Preprocess text data for NLP\n",
    "- Build and train a Bi-directional LSTM-CRF model using Keras and Tensorflow\n",
    "- Evaluate our model on the test set\n",
    "- Run the model on your own sentences!\n",
    "\n",
    "#### Instructions\n",
    "- To execute a code cell, click on the cell and press `Shift + Enter` (shortcut for Run).\n",
    "- To learn more about Workspaces, check out the [Getting Started Notebook](get_started_workspace.ipynb).\n",
    "- **Tip**: *Feel free to try this Notebook with your own data and on your own super awesome named entity recognition task.*\n",
    "\n",
    "Now, let's get started! üöÄ\n",
    "\n",
    "### Initial Setup\n",
    "Let's start by importing some packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install extra-dependencies\n",
    "! pip -q install git+https://www.github.com/keras-team/keras-contrib.git sklearn-crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters\n",
    "We'll set the hyperparameters for training our model. If you understand what they mean, feel free to play around - otherwise, we recommend keeping the defaults for your first run üôÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams if GPU is available\n",
    "if tf.test.is_gpu_available():\n",
    "    BATCH_SIZE = 512  # Number of examples used in each iteration\n",
    "    EPOCHS = 5  # Number of passes through entire dataset\n",
    "    MAX_LEN = 75  # Max length of review (in words)\n",
    "    EMBEDDING = 40  # Dimension of word embedding vector\n",
    "\n",
    "    \n",
    "# Hyperparams for CPU training\n",
    "else:\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 5\n",
    "    MAX_LEN = 75\n",
    "    EMBEDDING = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The movie ner dataset is already attached to your workspace (if you want to attach your own data, [check out our docs](https://docs.floydhub.com/guides/workspace/#attaching-floydhub-datasets)).\n",
    "\n",
    "Let's take a look at data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences:  47959\n",
      "Number of words in the dataset:  35178\n",
      "Tags: ['O', 'I-nat', 'I-eve', 'B-nat', 'I-art', 'B-gpe', 'I-org', 'I-gpe', 'B-per', 'I-per', 'B-eve', 'I-tim', 'B-geo', 'I-geo', 'B-org', 'B-art', 'B-tim']\n",
      "Number of Labels:  17\n",
      "What the dataset looks like:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS    Tag\n",
       "0  Sentence: 1      Thousands  NNS      O\n",
       "1  Sentence: 1             of   IN      O\n",
       "2  Sentence: 1  demonstrators  NNS      O\n",
       "3  Sentence: 1           have  VBP      O\n",
       "4  Sentence: 1        marched  VBN      O\n",
       "5  Sentence: 1        through   IN      O\n",
       "6  Sentence: 1         London  NNP  B-geo\n",
       "7  Sentence: 1             to   TO      O\n",
       "8  Sentence: 1        protest   VB      O\n",
       "9  Sentence: 1            the   DT      O"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"/floyd/input/ner/ner_dataset.csv\", encoding=\"latin1\")\n",
    "data = data.fillna(method=\"ffill\")\n",
    "\n",
    "print(\"Number of sentences: \", len(data.groupby(['Sentence #'])))\n",
    "\n",
    "words = list(set(data[\"Word\"].values))\n",
    "n_words = len(words)\n",
    "print(\"Number of words in the dataset: \", n_words)\n",
    "\n",
    "tags = list(set(data[\"Tag\"].values))\n",
    "print(\"Tags:\", tags)\n",
    "n_tags = len(tags)\n",
    "print(\"Number of Labels: \", n_tags)\n",
    "\n",
    "print(\"What the dataset looks like:\")\n",
    "# Show the first 10 rows\n",
    "data.head(n=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is what a sentence looks like:\n",
      "[('Thousands', 'NNS', 'O'), ('of', 'IN', 'O'), ('demonstrators', 'NNS', 'O'), ('have', 'VBP', 'O'), ('marched', 'VBN', 'O'), ('through', 'IN', 'O'), ('London', 'NNP', 'B-geo'), ('to', 'TO', 'O'), ('protest', 'VB', 'O'), ('the', 'DT', 'O'), ('war', 'NN', 'O'), ('in', 'IN', 'O'), ('Iraq', 'NNP', 'B-geo'), ('and', 'CC', 'O'), ('demand', 'VB', 'O'), ('the', 'DT', 'O'), ('withdrawal', 'NN', 'O'), ('of', 'IN', 'O'), ('British', 'JJ', 'B-gpe'), ('troops', 'NNS', 'O'), ('from', 'IN', 'O'), ('that', 'DT', 'O'), ('country', 'NN', 'O'), ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "class SentenceGetter(object):\n",
    "    \"\"\"Class to Get the sentence in this format:\n",
    "    [(Token_1, Part_of_Speech_1, Tag_1), ..., (Token_n, Part_of_Speech_1, Tag_1)]\"\"\"\n",
    "    def __init__(self, data):\n",
    "        \"\"\"Args:\n",
    "            data is the pandas.DataFrame which contains the above dataset\"\"\"\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"POS\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        \"\"\"Return one sentence\"\"\"\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "getter = SentenceGetter(data)\n",
    "sent = getter.get_next()\n",
    "print('This is what a sentence looks like:')\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the output Cell above, each sentence in the dataset is represented as a list of tuple: [`(Token_1, PoS_1, Tag_1)`, ..., `(Token_n, PoS_n, Tag_n)`]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get all the sentences\n",
    "sentences = getter.sentences\n",
    "\n",
    "# Plot sentence by lenght\n",
    "plt.hist([len(s) for s in sentences], bins=50)\n",
    "plt.title('Token per sentence')\n",
    "plt.xlabel('Len (number of token)')\n",
    "plt.ylabel('# samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Before feeding the data into the model, we have to preprocess the text.\n",
    "\n",
    "- We will use the `word2idx` dictionary to convert each word to a corresponding integer ID and the `tag2idx` to do the same for the labels. Representing words as integers saves a lot of memory!\n",
    "- In order to feed the text into our Bi-LSTM-CRF, all texts should be the same length. We ensure this using the `sequence.pad_sequences()` method and `MAX_LEN` variable. All texts longer than `MAX_LEN` are truncated and shorter texts are padded to get them to the same length.\n",
    "\n",
    "The *Tokens per sentence* plot (see above) is useful for setting the `MAX_LEN` training hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word Obama is identified by the index: 9881\n",
      "The labels B-geo(which defines Geopraphical Enitities) is identified by the index: 13\n",
      "Raw Sample:  Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
      "Raw Label:  O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O\n",
      "After processing, sample: [16817  7825 10253 17489    66 10783  7144 32555  6507  8582  7721 25544\n",
      " 28446  2656 25382  8582 32363  7825 23884 21607 15364 29850 18029  8610\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0]\n",
      "After processing, labels: [[0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary Key:word -> Value:token_index\n",
    "# The first 2 entries are reserved for PAD and UNK\n",
    "word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
    "word2idx[\"UNK\"] = 1 # Unknown words\n",
    "word2idx[\"PAD\"] = 0 # Padding\n",
    "\n",
    "# Vocabulary Key:token_index -> Value:word\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "\n",
    "# Vocabulary Key:Label/Tag -> Value:tag_index\n",
    "# The first entry is reserved for PAD\n",
    "tag2idx = {t: i+1 for i, t in enumerate(tags)}\n",
    "tag2idx[\"PAD\"] = 0\n",
    "\n",
    "# Vocabulary Key:tag_index -> Value:Label/Tag\n",
    "idx2tag = {i: w for w, i in tag2idx.items()}\n",
    "\n",
    "print(\"The word Obama is identified by the index: {}\".format(word2idx[\"Obama\"]))\n",
    "print(\"The labels B-geo(which defines Geopraphical Enitities) is identified by the index: {}\".format(tag2idx[\"B-geo\"]))\n",
    "\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# Convert each sentence from list of Token to list of word_index\n",
    "X = [[word2idx[w[0]] for w in s] for s in sentences]\n",
    "# Padding each sentence to have the same lenght\n",
    "X = pad_sequences(maxlen=MAX_LEN, sequences=X, padding=\"post\", value=word2idx[\"PAD\"])\n",
    "\n",
    "# Convert Tag/Label to tag_index\n",
    "y = [[tag2idx[w[2]] for w in s] for s in sentences]\n",
    "# Padding each sentence to have the same lenght\n",
    "y = pad_sequences(maxlen=MAX_LEN, sequences=y, padding=\"post\", value=tag2idx[\"PAD\"])\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "# One-Hot encode\n",
    "y = [to_categorical(i, num_classes=n_tags+1) for i in y]  # n_tags+1(PAD)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.1)\n",
    "X_tr.shape, X_te.shape, np.array(y_tr).shape, np.array(y_te).shape\n",
    "\n",
    "print('Raw Sample: ', ' '.join([w[0] for w in sentences[0]]))\n",
    "print('Raw Label: ', ' '.join([w[2] for w in sentences[0]]))\n",
    "print('After processing, sample:', X[0])\n",
    "print('After processing, labels:', y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We will implement a model similar to Zhiheng Huang‚Äôs [Bidirectional LSTM-CRF Models for Sequence Tagging](https://arxiv.org/pdf/1508.01991v1.pdf).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/floydhub/named-entity-recognition-template/master/images/bilstm-crf.png\" width=\"400\" height=\"400\" align=\"center\"/>\n",
    "\n",
    "*Image from [the paper](https://arxiv.org/pdf/1508.01991v1.pdf)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 75)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 75, 20)            703600    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 75, 100)           28400     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 75, 50)            5050      \n",
      "_________________________________________________________________\n",
      "crf_1 (CRF)                  (None, 75, 18)            1278      \n",
      "=================================================================\n",
      "Total params: 738,328\n",
      "Trainable params: 738,328\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from keras_contrib.layers import CRF\n",
    "\n",
    "# Model definition\n",
    "input = Input(shape=(MAX_LEN,))\n",
    "model = Embedding(input_dim=n_words+2, output_dim=EMBEDDING, # n_words + 2 (PAD & UNK)\n",
    "                  input_length=MAX_LEN, mask_zero=True)(input)  # default: 20-dim embedding\n",
    "model = Bidirectional(LSTM(units=50, return_sequences=True,\n",
    "                           recurrent_dropout=0.1))(model)  # variational biLSTM\n",
    "model = TimeDistributed(Dense(50, activation=\"relu\"))(model)  # a dense layer as suggested by neuralNer\n",
    "crf = CRF(n_tags+1)  # CRF layer, n_tags+1(PAD)\n",
    "out = crf(model)  # output\n",
    "\n",
    "model = Model(input, out)\n",
    "model.compile(optimizer=\"rmsprop\", loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Evaluate\n",
    "\n",
    "The Training is defined at the beginning by the type of instance on which runs:\n",
    "\n",
    "- On CPU machine: 25 minutes for 5 epochs.\n",
    "- On GPU machine: 5 minute for 5 epochs.\n",
    "\n",
    "*Note*: Accuracy isn't the best metric to choose for evaluating this type of task because most of the time it will correctly predict '**O**' or '**PAD**' without identifing the important Tags, which are the ones we are interested in. So after training for some epochs, we can monitor the **precision**, **recall** and **f1-score** for each of the Tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 38846 samples, validate on 4317 samples\n",
      "Epoch 1/5\n",
      "38846/38846 [==============================] - 254s 7ms/step - loss: 9.1008 - acc: 0.9034 - val_loss: 9.0202 - val_acc: 0.9517\n",
      "Epoch 2/5\n",
      "38846/38846 [==============================] - 247s 6ms/step - loss: 8.8708 - acc: 0.9589 - val_loss: 8.9733 - val_acc: 0.9625\n",
      "Epoch 3/5\n",
      "38846/38846 [==============================] - 246s 6ms/step - loss: 8.8402 - acc: 0.9668 - val_loss: 8.9584 - val_acc: 0.9648\n",
      "Epoch 4/5\n",
      "38846/38846 [==============================] - 248s 6ms/step - loss: 8.8287 - acc: 0.9703 - val_loss: 8.9542 - val_acc: 0.9654\n",
      "Epoch 5/5\n",
      "38846/38846 [==============================] - 249s 6ms/step - loss: 8.8227 - acc: 0.9721 - val_loss: 8.9500 - val_acc: 0.9681\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_tr, np.array(y_tr), batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "                    validation_split=0.1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval\n",
    "pred_cat = model.predict(X_te)\n",
    "pred = np.argmax(pred_cat, axis=-1)\n",
    "y_te_true = np.argmax(y_te, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-art       0.00      0.00      0.00        52\n",
      "      B-eve       1.00      0.33      0.50        15\n",
      "      B-geo       0.86      0.90      0.88      3677\n",
      "      B-gpe       0.97      0.93      0.95      1570\n",
      "      B-nat       0.00      0.00      0.00        22\n",
      "      B-org       0.74      0.73      0.74      2012\n",
      "      B-per       0.82      0.82      0.82      1726\n",
      "      B-tim       0.93      0.87      0.90      2063\n",
      "      I-art       0.00      0.00      0.00        36\n",
      "      I-eve       0.00      0.00      0.00        11\n",
      "      I-geo       0.83      0.77      0.80       696\n",
      "      I-gpe       0.92      0.57      0.71        21\n",
      "      I-nat       0.00      0.00      0.00         8\n",
      "      I-org       0.74      0.81      0.77      1657\n",
      "      I-per       0.87      0.90      0.88      1835\n",
      "      I-tim       0.84      0.73      0.78       642\n",
      "          O       0.99      0.99      0.99     88839\n",
      "        PAD       1.00      1.00      1.00    254818\n",
      "\n",
      "avg / total       0.99      0.99      0.99    359700\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "\n",
    "# Convert the index to tag\n",
    "pred_tag = [[idx2tag[i] for i in row] for row in pred]\n",
    "y_te_true_tag = [[idx2tag[i] for i in row] for row in y_te_true] \n",
    "\n",
    "report = flat_classification_report(y_pred=pred_tag, y_true=y_te_true_tag)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate some samples in the test set. (At each execution it will test on a different sample)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample number 2106 of 4796 (Test Set)\n",
      "Word           ||True ||Pred\n",
      "==============================\n",
      "He             : O     O\n",
      "said           : O     O\n",
      ",              : O     O\n",
      "however        : O     O\n",
      ",              : O     O\n",
      "the            : O     O\n",
      "world          : O     O\n",
      "body           : O     O\n",
      "remains        : O     O\n",
      "committed      : O     O\n",
      "to             : O     O\n",
      "work           : O     O\n",
      "for            : O     O\n",
      "peace          : O     O\n",
      "in             : O     O\n",
      "such           : O     O\n",
      "places         : O     O\n",
      "as             : O     O\n",
      "Lebanon        : B-geo B-geo\n",
      ",              : O     O\n",
      "Darfur         : B-geo B-geo\n",
      ",              : O     O\n",
      "Haiti          : B-geo B-geo\n",
      "and            : O     O\n",
      "Iraq           : B-geo B-geo\n",
      ".              : O     O\n"
     ]
    }
   ],
   "source": [
    "i = np.random.randint(0,X_te.shape[0]) # choose a random number between 0 and len(X_te)\n",
    "p = model.predict(np.array([X_te[i]]))\n",
    "p = np.argmax(p, axis=-1)\n",
    "true = np.argmax(y_te[i], -1)\n",
    "\n",
    "print(\"Sample number {} of {} (Test Set)\".format(i, X_te.shape[0]))\n",
    "# Visualization\n",
    "print(\"{:15}||{:5}||{}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "print(30 * \"=\")\n",
    "for w, t, pred in zip(X_te[i], true, p[0]):\n",
    "    if w != 0:\n",
    "        print(\"{:15}: {:5} {}\".format(words[w-2], idx2tag[t], idx2tag[pred]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's your turn\n",
    "\n",
    "Test out the model you just trained. Run the code Cell below and type your reviews in the widget, Have fun!üéâ\n",
    "\n",
    "Here are some inspirations:\n",
    "\n",
    "- Obama was the president of USA.\n",
    "- The 1906 San Francisco earthquake was the biggest earthquake that has ever hit San Francisco on April 18, 1906\n",
    "- Next Monday is Christmas!\n",
    "\n",
    "Can you do better? Play around with the model hyperparameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c20be87443af4311bc10820fb1e51639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Textarea(value='', description='sentence', placeholder='Type your sentence here'), Butto‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact_manual\n",
    "from ipywidgets import widgets\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Custom Tokenizer\n",
    "re_tok = re.compile(f'([{string.punctuation}‚Äú‚Äù¬®¬´¬ª¬Æ¬¥¬∑¬∫¬Ω¬æ¬ø¬°¬ß¬£‚Ç§‚Äò‚Äô])')\n",
    "def tokenize(s): return re_tok.sub(r' \\1 ', s).split()\n",
    "    \n",
    "def get_prediction(sentence):\n",
    "    test_sentence = tokenize(sentence) # Tokenization\n",
    "    # Preprocessing\n",
    "    x_test_sent = pad_sequences(sequences=[[word2idx.get(w, 0) for w in test_sentence]],\n",
    "                            padding=\"post\", value=word2idx[\"PAD\"], maxlen=MAX_LEN)\n",
    "    # Evaluation\n",
    "    p = model.predict(np.array([x_test_sent[0]]))\n",
    "    p = np.argmax(p, axis=-1)\n",
    "    # Visualization\n",
    "    print(\"{:15}||{}\".format(\"Word\", \"Prediction\"))\n",
    "    print(30 * \"=\")\n",
    "    for w, pred in zip(test_sentence, p[0]):\n",
    "        print(\"{:15}: {:5}\".format(w, idx2tag[pred]))\n",
    "\n",
    "interact_manual(get_prediction, sentence=widgets.Textarea(placeholder='Type your sentence here'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Saving Vocab\n",
    "with open('models/word_to_index.pickle', 'wb') as handle:\n",
    "    pickle.dump(word2idx, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    " \n",
    "# Saving Vocab\n",
    "with open('models/tag_to_index.pickle', 'wb') as handle:\n",
    "    pickle.dump(tag2idx, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# Saving Model Weight\n",
    "model.save_weights('models/lstm_crf_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### That's all folks - don't forget to shutdown your workspace once you're done üôÇ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
